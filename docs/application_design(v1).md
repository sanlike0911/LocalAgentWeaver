
## アプリケーション設計書：`LocalAgentWeaver(ローカル・エージェント・ウィーバー)` (MVP v0.1)

### 1. プロジェクト概要

#### 1.1. 目的
ローカル環境で安全かつ無料で動作する、ナレッジベース拡張型のAIチャットアプリケーションを構築する。ユーザーはプライベートなファイル群をAIに学習させ、専門的な対話を行うことができる。

#### 1.2. コア機能 (MVP)
-   **プロジェクト管理**: 対話のコンテキスト（チャット履歴、ナレッジベース）をプロジェクト単位で管理・切り替えできる。
-   **ナレッジベース構築**: プロジェクトごとにドキュメントファイル（PDF, TXT）をアップロードし、AIの知識源として利用できる。
-   **RAG (検索拡張生成) チャット**: ナレッジベースの内容を基に、出典を明示した回答をAIが生成する。
-   **ローカルLLM利用**: Ollamaを通じて、ローカルPC上でLLMを動作させる。

### 2. 技術スタック

| 役割 | ツール/ライブラリ | バージョン (推奨) | 目的 |
| :--- | :--- | :--- | :--- |
| アプリケーションフレームワーク | `chainlit` | `^1.0.0` | UI構築、フロント/バックエンド連携 |
| LLMオーケストレーション | `langchain` | `^0.1.0` | RAGパイプライン、LLM連携 |
| ローカルLLM実行環境 | `Ollama` | 最新版 | Llama 3等のLLMをローカルで実行 |
| ベクトルデータベース | `chromadb` | `^0.4.0` | ドキュメントベクトルの保存・検索 |
| LLM/Embedding連携 | `langchain-community` | `^0.0.15` | Ollama, ChromaDBとの連携 |
| ドキュメントローダー | `pypdf`, `unstructured` | 最新版 | PDF, TXTファイルの読み込み |
| 開発言語 | `Python` | `3.10+` | アプリケーション全体の開発言語 |

### 3. アーキテクチャ

```
+-----------------------------------------------------------------------------+
| [ User (Web Browser) ]                                                      |
|   - Chainlit UI                                                             |
+--------------------------------<-- WebSocket -->----------------------------+
                                        |
+-----------------------------------------------------------------------------+
| [ Backend: Python Application (app.py) ]                                    |
|   +-- Chainlit Server                                                       |
|   |    - @cl.on_chat_start (プロジェクト選択)                                |
|   |    - @cl.on_message (メインの処理)                                       |
|   |    - cl.Action (各種操作)                                                |
|   |                                                                         |
|   +-- Project Manager (modules/project_manager.py)                          |
|   |    - SQLiteデータベースへのCRUD操作                                      |
|   |                                                                         |
|   +-- RAG Engine (modules/rag_engine.py)                                    |
|   |    - LangChain RAG Chain                                                |
|   |    - Document Loading, Splitting, Embedding                             |
|   |                                                                         |
|   +-----------+--------------+------------------+---------------------------+
|               |              |                  |
|               v              v                  v
|   [ LangChain Community Integrations ]          |
|       - ChatOllama                              |
|       - OllamaEmbeddings                        |
|       - Chroma                                  |
|               |                                 |
+---------------+---------------------------------+---------------------------+
                |                                 |
                v                                 v
+-----------------------------+     +-----------------------------------------+
| [ Ollama Service ]          |     | [ ChromaDB (Vector Store) ]             |
|   - LLM (e.g., Llama 3 8B)  |     |   - 各プロジェクトのベクトルデータを永続化  |
|   - Embedding Model         |     |                                         |
+-----------------------------+     +-----------------------------------------+

```

### 4. UI/UXフロー (Chainlit上での実現)

1.  **アプリ起動 (`@cl.on_chat_start`)**:
    -   システムメッセージ「どのプロジェクトで作業しますか？」を表示。
    -   既存のプロジェクトリストをDBから取得し、`cl.Action`ボタンとして表示。
    -   「+ 新規プロジェクトを作成」ボタンも表示。
2.  **プロジェクト選択**:
    -   ユーザーがプロジェクトボタンをクリックすると、そのプロジェクトIDがユーザセッション (`cl.user_session`) に保存される。
    -   「新規作成」が押された場合、`cl.AskUserMessage`でプロジェクト名を尋ね、DBに新規登録後、そのIDをセッションに保存。
    -   システムメッセージ「プロジェクト『{プロジェクト名}』を開始します。ファイルをアップロードしてください。」を表示。
3.  **ファイルアップロード (`@cl.on_message`)**:
    -   ユーザーがファイルをドラッグ＆ドロップすると、`cl.Message`の`elements`にファイル情報が含まれる。
    -   ファイルを受け取り、バックグラウンドでRAG Engineが処理（ベクトル化してDBに保存）。
    -   システムメッセージ「『{ファイル名}』をナレッジベースに追加しました。」を表示。
4.  **チャット (`@cl.on_message`)**:
    -   ユーザーからのテキストメッセージを受け取る。
    -   RAG Engineを呼び出し、セッション内のプロジェクトIDに対応するナレッジベースから関連情報を検索し、LLMに渡して回答を生成。
    -   生成された回答と出典情報をメッセージとして表示。
5.  **その他の操作 (`cl.Action`)**:
    -   チャット入力欄の横にアクションボタンを常時表示。
    -   **「ナレッジ管理」**: クリックすると、現在のプロジェクトのファイル一覧と「削除」ボタンをメッセージとして表示する。
    -   **「プロジェクト切替」**: クリックすると、`@cl.on_chat_start` と同様のプロジェクト選択メッセージを再度表示する。

### 5. データモデル (SQLite)

-   **`projects` テーブル**

| カラム名 | データ型 | 説明 |
| :--- | :--- | :--- |
| `id` | INTEGER | 主キー (自動インクリメント) |
| `name` | TEXT | プロジェクト名 (UNIQUE) |
| `created_at` | TIMESTAMP | 作成日時 |

*Note: ChromaDBはコレクション名としてプロジェクトID (`project_{id}`) を使用することで、どのベクトルデータがどのプロジェクトに属するかを管理する。*

### 6. Phase 1: MVP開発 (Walk)

開発をいくつかの大きなタスク（エピック）に分け、さらに具体的なサブタスクに分解しました。これにより、何から手をつけるべきかが明確になります。

#### 1. 環境構築 (約0.5日)
-   [ ] Python開発環境 (venv)のセットアップするため、２つのスクリプトを作成（本番環境構築スクリプト、開発環境構築スクリプト）
    -   [ ] `chainlit`, `langchain`, `ollama`, `chromadb-client` 等、必要なライブラリのインストール
    -   [ ] Ollamaをインストールし、動作確認として `llama3` モデルをプル (`ollama run llama3`)

#### 2. 基本的なChainlitチャットアプリの作成 (約0.5日)
-   [ ] Chainlitの "Hello World" を実行 (`@cl.on_message`)
-   [ ] LangChainとOllamaを連携させ、Chainlit上でローカルLLMからの応答を表示する

#### 3. プロジェクト管理機能の実装 (約1.5日)
-   [ ] SQLiteでプロジェクト情報を管理するテーブルを設計・作成
-   [ ] (`@cl.on_chat_start`) アプリ起動時にSQLiteからプロジェクトリストを読み込む
-   [ ] プロジェクトリストを `cl.Action` ボタンとして表示する
-   [ ] 「+ 新規作成」ボタンが押されたら、`cl.AskUserMessage` でプロジェクト名を質問し、DBに保存する
-   [ ] ユーザーが選択したプロジェクト情報をChainlitのユーザセッション (`cl.user_session`) に保存する

#### 4. ナレッジベース (RAG) 機能の実装 (約2.5日)
-   [ ] (`@cl.on_message`) ファイルアップロードのロジックを実装する (Chainlitはファイルアップロードをサポート)
-   [ ] アップロードされたファイルをLangChainの `DocumentLoader` で読み込む (PDF, TXT対応)
-   [ ] 読み込んだドキュメントをLangChainの `TextSplitter` で分割する
-   [ ] 分割したテキストを `OllamaEmbeddings` でベクトル化する
-   [ ] ベクトルデータをChromaDBに保存する（プロジェクトIDと紐付けて管理）
-   [ ] ユーザーの質問に対し、ChromaDBから関連文書を検索し、LLMにコンテキストとして渡すRAGチェーンをLangChainで構築する
-   [ ] LLMの応答から出典情報を取得し、ChainlitのUI上に表示する

#### 5. 高度なUI/UX機能の実装 (約1日)
-   [ ] チャット入力欄に `cl.Action` を追加（「プロジェクト切替」「ナレッジ管理」）
-   [ ] 「ナレッジ管理」アクションが実行されたら、現在のプロジェクトのファイル一覧と削除ボタンを表示するロジックを実装
-   [ ] LLMの思考プロセス（ストリーミング応答）をリアルタイムで表示するように設定

#### 6. 全体調整とテスト (約1日)
-   [ ] プロジェクト切り替え時にチャット履歴やナレッジベースが正しくリセット/ロードされるかテスト
-   [ ] ファイルのアップロードからRAG応答までの一連の流れをテスト
-   [ ] 簡単なエラーハンドリングを追加（例：対応していないファイル形式など）
-   [ ] READMEファイル（使い方、セットアップ方法）の作成

### 7. MVP以降の拡張プラン (ロードマップ)

本アプリケーションは、MVPを成功させた後、以下のステップで段階的に機能を拡張し、より高度で自律的なAI Agentsシステムへと進化させることを目指します。

#### Phase 2: Multi-Agentシステムの導入 (Walk)
*目的: ユーザーが定義した複数のAI Agentが協調し、単一のAIでは解決が難しい複雑なタスクに取り組むことを可能にする。*

-   **機能追加**:
    -   **Agents作成・管理機能**:
        -   GUIを通じて、プロジェクトごとに複数のAgentを定義できる機能。
        -   各Agentには「名前」「役割（システムプロンプト）」を設定できる。
    -   **Agentチーム実行モード**:
        -   チャット開始時に「シングルチャットモード」か「Agentチームモード」かを選択できる。
        -   チームモードでは、ユーザーの指示を「マネージャーAgent」が解釈し、各専門Agentにタスクを割り振り、自律的に処理を進行させる。
-   **UI/UXの拡張**:
    -   ChainlitのUI上に「Agents」タブを追加し、Agentの作成・編集・削除を行う管理画面を設ける。
    -   Agentの思考プロセスやAgent間の対話をリアルタイムで可視化し、ユーザーが進捗を把握できるようにする（Chainlitの得意分野）。
-   **技術的拡張**:
    -   `LangChain Agents` (CrewAI, AutoGenなども検討) を導入し、Agentの実行と連携を管理する`Agent Executor`を実装する。
    -   SQLiteに`agents`テーブルを追加し、Agentの定義を永続化する。

#### Phase 3: Agentの能力拡張 (Run)
*目的: Agentがテキスト生成だけでなく、外部環境と連携する「ツール」を使用できるようにし、実行可能なタスクの幅を広げる。*

-   **機能追加**:
    -   **ツール利用機能**: Agentが特定のツールを呼び出せるようになる。
        -   **Web検索ツール**: 最新の情報をインターネットから取得し、回答に反映させる。
        -   **ファイルI/Oツール**: Agentの生成したレポートや分析結果を、ローカルにMarkdownやCSVファイルとして書き出す。
        -   **コード実行ツール (上級者向け)**: Pythonコードを安全なサンドボックス環境で実行し、データ分析や計算を行う（セキュリティ対策が必須）。
-   **UI/UXの拡張**:
    -   Agent作成画面で、そのAgentが使用可能なツールを選択できるチェックボックスを追加する。
    -   チャットログに、どのAgentがどのツールを使用したかを示すアイコンやログを表示する。
-   **技術的拡張**:
    -   LangChainの`Tool`や`Toolkit`の仕組みを活用し、各機能をAgentに提供する。
    -   インターネットアクセスやファイルシステム操作に伴うセキュリティリスクを考慮した実装を行う。

#### Phase 4: ハイブリッド化とエコシステム (Fly)
*目的: ローカル環境のプライバシー性と、クラウドAIの高性能・高機能性を両立させ、ユーザーが最適な環境を選択できるハイブリッドモデルを実現する。*

-   **機能追加**:
    -   **クラウドLLM対応**:
        -   設定画面でOpenAI, Anthropic, GoogleなどのAPIキーを登録できるようにする。
        -   プロジェクトごと、あるいはチャットごとに使用するLLM（ローカル/クラウド）を切り替えられる機能。
    -   **Agentテンプレート共有機能**:
        -   「新規事業開発チーム」「ソフトウェア設計チーム」といった、事前定義されたAgentチームのテンプレートを提供する。
        -   (将来的には) ユーザーが作成したAgentチームをエクスポート/インポートし、コミュニティで共有できる仕組み。
-   **UI/UXの拡張**:
    -   LLM選択ドロップダウンに、設定済みのクラウドAPIモデルを表示する。
-   **技術的拡張**:
    -   LangChainのアーキテクチャはLLMの切り替えを容易にするため、API呼び出し部分のロジックを追加するだけで対応可能。

---
