## **LocalAgentWeaver システム開発設計書 兼 Claude Code実装指示書**

### 1. プロジェクト概要

#### 1.1. プロジェクト名
LocalAgentWeaver

#### 1.2. コンセプト
ローカル環境で動作するLLM（Ollama/LM Studio）と連携し、チームでのドキュメントベースの対話やアイデア創出を支援する、セキュアなAIエージェントプラットフォーム。ユーザーはプロジェクトを作成し、ドキュメントをアップロードし、AIエージェントからなる「チーム」と対話することで、高度な情報分析やコンテンツ生成を行う。

#### 1.3. 主要機能
- **ユーザー認証・管理**
- **プロジェクト管理**
- **LLM機能**
  - ローカルLLM（Ollama/LM Studio）との連携機能
  - **LLMモデル自動管理機能**（未インストールモデルの自動ダウンロード・インストール）
  - 応答時間測定・表示機能
- **インテリジェントチャット機能**
  - AIエージェントチームとの対話インターフェース
  - リアルタイムチャット履歴管理
  - プロバイダー・モデル選択機能
  - **RAG統合チャット処理フロー:**
    1. **質問受信**: ユーザーがフロントエンドで質問を送信
    2. **ベクトル化**: 質問を埋め込みモデルでベクトルに変換
    3. **類似度検索**: 現在のプロジェクトのベクトルデータベースに対して類似度検索を実行
    4. **コンテキスト拡張**: 検索されたテキストチャンクを元の質問と組み合わせてプロンプトを拡張
    5. **LLM問い合わせ**: 拡張されたプロンプトを選択されたLLMに送信
    6. **回答生成**: AIエージェントチームが関連情報に基づいて回答を生成
    7. **結果表示**: 生成された回答を出典元ドキュメントの引用情報とともに表示
- **RAG機能（Retrieval-Augmented Generation）**
  - **ドキュメント管理**
    - ファイルアップロード（プロジェクト単位: default 100件、ファイルサイズ上限: default 30MB）
    - 対応ファイル形式：**TXT、MD、PDF、EXCEL、WORD、POWERPOINT**
    - 有効/無効切り替え機能
  - **LlamaIndex統合RAG処理パイプライン**
    - **SimpleDirectoryReader**: 各ファイル形式に最適化されたコンテンツ抽出
    - **NodeParser**: 意味単位での効率的なテキスト分割・チャンキング
    - **VectorStoreIndex**: LanceDB/Qdrant統合による高速ベクトルインデックス
    - **RetrieverQueryEngine**: 高精度な類似度検索・コンテキスト生成
    - **高速アクセス最適化**: LlamaIndexとベクトルDBの統合による<100ms応答時間
- **AIエージェントチーム管理**
  - チーム作成・編集・選択機能
  - エージェント構成管理
  - プリセットチームテンプレート
- **プロジェクト共有機能**

#### 1.4. 実装上の共通指針（コーディング規約）

- **開発手法:** **テスト駆動開発（TDD）**を厳守する。機能実装前にテストコードを作成すること。
- **認証:** メールアドレスとパスワードによる認証。認証トークンには**JWT**を使用する。
- **LLMプロバイダー:** **Ollama**および**LM Studio**に対応。設定ファイル (`.env`) とUIの両方から接続先を切り替え可能な設計とする。
- **LLMモデル管理:** 未インストールモデルの自動検出・ダウンロード・インストール機能を提供。進行状況をリアルタイム表示する。
- **設定管理:** DB接続情報、LLM接続先、ファイルアップロード上限（デフォルト30MB）などの設定値は、環境変数 (`.env`) で管理する。
- **デプロイ:** **Docker**および**docker-compose**を前提とする。Windows/Linuxの両環境で動作すること。
- **ログ:** **構造化ログ**を出力する。リクエストIDなどを付与し、処理の追跡を容易にすること。
- **コード品質:** 可読性と保守性を最優先する。型ヒントを徹底し、静的解析ツールを導入する。
- **セキュリティ:** ファイルアップロード時のMIMEタイプ検証、APIへのCORS設定とレート制限、ユーザー入力のサニタイズ・バリデーションを行う。

---

### 2. システムアーキテクチャ

#### 2.1. システム構成図
LocalAgentWeaverは、フロントエンド、バックエンド、データベース群（メタデータ、ベクトル、キャッシュ）、そしてユーザーのローカル環境で動作するLLM実行環境から構成されます。RAG機能の高速化のため、専用ベクトルデータベースを導入します。

```
 +-----------+      +----------------------+      +------------------+
 |           | HTTP |                      |      |                  |
 |  ユーザー   |----->|   Frontend (Next.js) |----->|  Backend (FastAPI) |
 | (ブラウザ)  |      | (UI/UX)              |      | (ビジネスロジック) |
 +-----------+      +-----------+----------+      +---------+--------+
                                |                          |
            (WebSocketリアルタイム通信)                      | (API Call)
                                |                          |
                                |          +---------------v----------------+
                                |          |                                |
                                |          |    LLM実行環境 (ローカル)      |
                                +--------->|     - Ollama                   |
                                           |     - LM Studio                |
                                           |                                |
                                           +--------------------------------+
                                                           ^
                                                           |
                       +-------------------+---------------+--------------------+
                       |                   |               |                    |
              +--------v---------+ +-------v--------+ +----v---------+ +-------v--------+
              |                  | |                | |              | |                |
              |    PostgreSQL    | | LanceDB/Qdrant | |    Redis     | |   Embedding    |
              |  (メタデータ     | |  (ベクトルDB)   | | (キャッシュ) | |    Service     |
              |   永続化)        | | (RAG高速検索)  | |              | | (ベクトル生成) |
              +------------------+ +----------------+ +--------------+ +----------------+
```

#### 2.2. 技術スタック
| カテゴリ | 技術選定 | 理由 |
| :--- | :--- | :--- |
| **バックエンド** | Python, FastAPI | LLMとの親和性が高く、高性能。WebSocketによるリアルタイム通信に対応。 |
| **フロントエンド** | TypeScript, React, Next.js | 型安全で堅牢な開発が可能。SSR/SSGによる高いパフォーマンスを実現。 |
| **メタデータDB** | PostgreSQL | ユーザー、プロジェクト、チーム等のリレーショナルデータ永続化。 |
| **ベクトルDB** | **LanceDB (開発環境) / Qdrant (本番環境)** | **RAG機能の高速類似度検索に特化。LanceDBは軽量で開発に適し、Qdrantは高性能・高可用性。** |
| **RAGフレームワーク** | **LlamaIndex** | **AnythingLLM実証済みのデータ中心RAGフレームワーク。FastAPI統合容易、高速インデックス・検索機能。** |
| **キャッシュ** | Redis | セッション管理、チャット履歴、モデル情報等の高速キャッシュ。 |
| **埋め込みサービス** | **sentence-transformers / Ollama Embeddings** | **ローカル実行可能な多言語対応埋め込みモデル。プライバシー保護とコスト最適化。** |
| **UIフレームワーク** | Tailwind CSS, Shadcn/ui | 高いカスタマイズ性とコンポーネント指向による効率的なUI開発。 |
| **LLM実行環境** | **Ollama / LM Studio (設定により切替可能)** | **ローカル環境でセキュアにLLMを実行。ユーザーの既存環境や好みに対応。** |
| **コンテナ化** | Docker, Docker Compose | 開発・本番環境の差異をなくし、ポータビリティを確保。 |
| **テスト** | Pytest (BE), Jest/Playwright (FE) | TDDを実践し、コード品質を担保するための標準的なテストフレームワーク。|

#### 2.3. `docker-compose.yml` の構成
RAG機能のためのベクトルデータベース群と埋め込みサービスを含む、完全なサービススタックを定義します。

```yaml
# docker-compose.yml の構成概要
version: '3.8'
services:
  # Core Application Services
  backend: ...
  frontend: ...
  
  # Database Layer
  postgresql: ...        # メタデータ永続化
  redis: ...             # キャッシュ・セッション管理
  
  # RAG Vector Database (開発環境はLanceDB、本番環境はQdrant)
  lancedb:
    image: lancedb/lancedb:latest
    ports:
      - "8000:8000"
    volumes:
      - ./lancedb-data:/data
    environment:
      - LANCE_DB_URI=/data
    profiles:
      - development
  
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant-data:/qdrant/storage
    profiles:
      - production

  # Embedding Service
  embedding-service:
    image: sentence-transformers/sentence-transformers
    ports:
      - "8001:8000"
    environment:
      - MODEL_NAME=all-MiniLM-L6-v2  # 多言語対応軽量モデル
    volumes:
      - ./embedding-models:/models

  # LLM実行環境（選択的起動）
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
    # GPUを利用する場合の推奨設定
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - ollama
```

#### 2.4. ファイル・ディレクトリ構造
RAG機能とベクトルデータベース統合を考慮したフィーチャーベースの構造を採用します。

```plaintext
LocalAgentWeaver/
├── backend/
│   ├── app/
│   │   ├── features/          # 機能ごとのモジュール
│   │   │   ├── auth/          # 認証機能
│   │   │   ├── projects/      # プロジェクト管理
│   │   │   ├── teams/         # チーム・エージェント管理
│   │   │   ├── chat/          # チャット機能
│   │   │   ├── documents/     # ドキュメント管理
│   │   │   └── rag/           # RAG機能（ベクトル化、検索）
│   │   ├── core/              # 共通コア機能
│   │   │   ├── config.py      # 設定管理
│   │   │   ├── database.py    # PostgreSQL接続
│   │   │   └── vector_db.py   # ベクトルDB接続（LanceDB/Qdrant）
│   │   ├── services/          # 外部サービス連携
│   │   │   ├── llamaindex_service.py # LlamaIndexによるRAG処理
│   │   │   ├── embedding_service.py  # 埋め込みサービス
│   │   │   ├── llm_service.py        # LLM連携
│   │   │   └── vector_service.py     # ベクトル操作（LanceDB/Qdrant）
│   │   ├── models/            # データモデル定義
│   │   │   ├── postgresql/    # SQLAlchemyモデル
│   │   │   └── vector/        # ベクトルデータモデル
│   │   └── tests/             # テストコード
│   ├── requirements.txt
│   └── Dockerfile
├── frontend/
│   ├── src/
│   │   ├── features/          # 機能ごとのコンポーネント
│   │   │   ├── auth/
│   │   │   ├── projects/
│   │   │   ├── teams/
│   │   │   ├── chat/
│   │   │   └── documents/     # ドキュメント管理UI
│   │   ├── components/ui/     # 共通UIコンポーネント
│   │   ├── hooks/             # 共通カスタムフック
│   │   └── utils/             # 共通ユーティリティ関数
│   ├── package.json
│   └── Dockerfile
├── data/                      # 永続化データ
│   ├── postgresql/            # PostgreSQLデータ
│   ├── lancedb-data/          # LanceDBベクトルデータ
│   ├── qdrant-data/           # Qdrantベクトルデータ
│   ├── embedding-models/      # 埋め込みモデル
│   └── ollama-data/           # Ollamaモデルデータ
├── scripts/                   # セットアップ用スクリプト
│   ├── setup-dev.sh
│   ├── setup-prod.sh
│   └── init-vector-db.sh      # ベクトルDB初期化
├── .env.example               # 環境変数テンプレート
└── docker-compose.yml         # サービス定義
```

---

### 3. UI/UX設計 (詳細仕様)

**基本方針:** 本システムのUIは、ドキュメントという「コンテキスト」を管理しながら、AIチームと「対話」することに最適化された設計とする。複雑な操作を排し、チーム選択や編集といった主要なワークフローを直感的かつ情報豊かに提供する。

#### 3.1. 画面構成図

```
                                      +--------------------------------+
                                      |                                |
[未ログイン] -------------------------> |  ログイン / 新規登録画面        |
                                      |  (LAW-SC-010)                  |
                                      +---------------+----------------+
                                                      | (認証成功)
                                                      |
                          +---------------------------v----------------------------+
                          |                                                        |
                          |      ダッシュボード (プロジェクト一覧画面)             |
                          |      (LAW-SC-100)                                      |
                          |                                                        |
                          +---------------------------+----------------------------+
                                                      | (プロジェクト選択)
                                                      |
+-----------------------------------------------------v------------------------------------------------------+
|                                                                                                              |
|                                       メイン画面 (チャット / プロジェクト編集)                               |
|                                       (LAW-SC-200)                                                           |
|                                                                                                              |
+-----------------------------------------------------+--------------------------------------------------------+
              | (左サイドバーのチーム名をクリック)              ^
              |                                               | (チーム選択後)
              v                                               |
+---------------------------+---------------------------------+---------------------------+
|                           |                                 |                           |
|  チーム選択モーダル       | <--(新規作成/編集)-->            |  チーム編集モーダル       |
|  (LAW-SC-215)             |                                 |  (LAW-SC-210)             |
|                           |                                 |                           |
+---------------------------+---------------------------------+---------------------------+
              ^                                               
              | (Modelボタンクリック)                         
              |                                               
+---------------------------+                               
|                           |                               
|  モデル選択モーダル       |                               
|  (LAW-SC-220)             |                               
|                           |                               
+---------------------------+
```

#### 3.2. 各画面仕様

##### 3.2.1. ログイン / 新規登録画面 - `LAW-SC-010`
*   **概要:** ユーザー認証を行うためのエントリーポイント。タブでログインと新規登録を切り替えるシンプルなフォーム。

##### 3.2.2. ダッシュボード（プロジェクト一覧）画面 - `LAW-SC-100`
*   **概要:** ユーザーが関わるプロジェクトを管理するハブ画面。プロジェクトをカード形式で一覧表示し、新規作成や検索が可能。

##### 3.2.3. メイン画面（チャット / プロジェクト編集） - `LAW-SC-200`
*   **概要:** アプリケーションの中核画面。3カラムレイアウトを採用。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------------+
    |  [←] プロジェクト名 - プロジェクト説明                                             |
    +--------------------------------------------------------------------------------+
    | Left Sidebar     |        Center Chat Area       |   Right Sidebar           |
    | (320px)          |         (Flex-1)               |   (256px)                 |
    |                  |                                |                           |
    | [ チーム選択 ]   |  [ AI Assistant ]  [ Provider ][ Model ]                  |
    | 👥 選択中のチーム    |   ┌─ Ollama ▼─┐   [ llama3 ]  | 📊 LLM接続状態            |
    | 説明文...         |   └─────────────┘             | ○ Ollama: 接続中         |
    | [ ⚙️ 設定 ]       |                                | 📋 利用可能モデル:        |
    |                  |  ┌─ Chat Messages Area ─────┐  |   llama3, codellama     |
    | [ ドキュメント ]     |  │ 💬 User: こんにちは       │  |                           |
    | [📄 Upload ]      |  │ 🤖 AI: こんにちは!         │  | 📚 引用・出典             |
    |                  |  │                            │  |                           |
    | 📁 Drop Zone     |  │ ...                        │  | AIの応答に関連する       |
    | ファイルをD&D...   |  │                            │  | ドキュメントの引用が     |
    |                  |  │                            │  | ここに表示されます       |
    | 📋 Document List |  └────────────────────────────┘  |                           |
    | ✅ doc1.pdf      |                                |                           |
    | ❌ doc2.txt      |  [ メッセージ入力欄...    ] [送信]  |                           |
    +------------------+--------------------------------+---------------------------+
    ```
*   **左サイドバー (320px):**
    *   **チーム選択エリア:** 現在のチーム名と説明を表示するクリック可能なエリア。クリックで**チーム選択モーダル (`LAW-SC-215`)** を開く。
    *   **ドキュメント管理:** ファイルアップロード、ドラッグ&ドロップエリア、コンテキストに含めるかの有効/無効切り替え。
*   **中央エリア (Flex-1):** 
    *   **ヘッダー:** LLMプロバイダー選択（Ollama/LM Studio）、モデル選択
    *   **チャット履歴:** ユーザーとAIの対話履歴
    *   **応答時間表示:** チャット入力からLLMの応答が完了するまでの経過時間をタイムスタンプと同一行に「21:15（応答時間：23.4秒）」の形式で表示。1秒未満は小数点表示、1分未満は秒のみ、1時間未満は分秒、1時間以上は時分秒で表示
    *   **プロンプト入力欄:** メッセージ入力と送信ボタン
*   **右サイドバー (256px):**
    *   **LLM接続状態:** 現在のプロバイダーの接続状況と利用可能モデル一覧
    *   **引用・出典表示:** AIの応答に関連するドキュメントの引用情報

##### 3.2.4. チーム選択モーダル - `LAW-SC-215`
*   **概要:** チームの概要を確認しながら、プロジェクトで使用するAIチームを選択するモーダル。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ チームを選択 ]                                                          |
    |--------------------------------------------------------------------------|
    |  [ 🔍 チームを検索... ]  [ + 新しいチームを作成 ]                          |
    |--------------------------------------------------------------------------|
    |  +--------------------------------------------------------------------+  |
    |  | [ 開発チーム ] (現在選択中)                             [ ⚙️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | 役割: ソフトウェア開発に関する質問に答える専門家チームです。         |  |
    |  | エージェント: [ シニアデベロッパー ] [ QAエンジニア ]                |  |
    |  +--------------------------------------------------------------------+  |
    |  ... (他のチームタイル) ...                                             |
    |--------------------------------------------------------------------------|
    |                                                         [ 閉じる ]       |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   **チームタイルクリック:** チームを選択しモーダルを閉じる。
    *   **`+ 新しいチームを作成` ボタン:** `LAW-SC-210`を新規作成モードで開く。
    *   **`⚙️` (編集) ボタン:** `LAW-SC-210`を編集モードで開く。

##### 3.2.5. チーム編集モーダル - `LAW-SC-210`
*   **概要:** AIエージェントを組み合わせて独自のチームを新規作成、または編集するモーダル。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ チームを編集 ]                                             [ × ]      |
    |--------------------------------------------------------------------------|
    |  チーム名: [ input: 開発チーム ]                                         |
    |  チームの役割・目的: [ textarea: このチームは... ]                       |
    |  ----------------------------------------------------------------------  |
    |  [ エージェント ]                                                        |
    |  +--------------------------------------------------------------------+  |
    |  | [エージェント 1] [🗑️] エージェント名: [ input ] 役割: [ textarea ]  |  |
    |  +--------------------------------------------------------------------+  |
    |  [ + エージェントを追加 ]                                                |
    |  ----------------------------------------------------------------------  |
    |  [ 他のチームからエージェントをコピー ]                                  |
    |  コピー元チーム: [ 事業戦略チーム ▼ ]                                    |
    |  +--------------------------------------------------------------------+  |
    |  | 役割: 新規事業の立案と市場分析を行います... (リードオンリー)         |  |
    |  +--------------------------------------------------------------------+  |
    |  エージェント: [ ✅ マーケター ] [ ✅ データアナリスト ]                 |
    |  [ 選択したエージェントをコピー ]                                        |
    |--------------------------------------------------------------------------|
    |  [ キャンセル ]                                    [ 保存 ]              |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   チームの基本情報（名称、役割）を定義。
    *   エージェントを動的に追加・削除。
    *   他のチームからエージェントを役割を確認しながらコピー。

##### 3.2.6. モデル選択モーダル - `LAW-SC-220`
*   **概要:** LLMモデルを選択・管理するためのモーダル。インストール済みモデルの選択、新しいモデルのインストール、モデルの削除が可能。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ Ollama モデル選択 ]                                      [ × ]      |
    |--------------------------------------------------------------------------|
    |                                                                          |
    |  [ インストール済みモデル ]                                              |
    |  +--------------------------------------------------------------------+  |
    |  | [ llama3.2:1b ] (現在選択中)                          [ 🗑️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | サイズ: 1.3GB　更新: 17時間前                                     |  |
    |  +--------------------------------------------------------------------+  |
    |  | [ codellama:7b ]                                       [ 🗑️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | サイズ: 3.8GB　更新: 2日前                                        |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |  [ 推奨モデル ]                                                          |
    |  +--------------------------------------------------------------------+  |
    |  | [ llama3 ]                                            [ ⬇️ インストール ] |
    |  |--------------------------------------------------------------------|  |
    |  | Meta の Llama 3 モデル                                             |  |
    |  +--------------------------------------------------------------------+  |
    |  | [ mistral ]                                           [ ⬇️ インストール ] |
    |  |--------------------------------------------------------------------|  |
    |  | Mistral AI の高性能モデル                                          |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |  [ インストール状況 ]                                                    |
    |  +--------------------------------------------------------------------+  |
    |  | [ gemma ] ⏳ インストール中...                                      |  |
    |  |--------------------------------------------------------------------|  |
    |  | ████████████████░░░░ 80% ダウンロード中...                        |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |--------------------------------------------------------------------------|
    |  [ 更新 ]                                              [ 閉じる ]       |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   **インストール済みモデル表示:** 現在利用可能なモデルを一覧表示。選択中のモデルを強調表示。
    *   **モデル選択:** クリックでモデルを選択・切り替え。
    *   **モデル削除:** 各モデルの削除ボタンで不要なモデルを削除（確認ダイアログ表示）。
    *   **推奨モデル表示:** プロバイダーごとの推奨モデルを表示し、ワンクリックでインストール開始。
    *   **リアルタイムインストール進捗:** インストール中のモデルの進捗をプログレスバーで表示。
    *   **インストール状況管理:** 複数モデルの同時インストールに対応し、各状況を個別に表示。
*   **状態管理:**
    *   **インストールタスク監視:** バックグラウンドで2秒間隔でインストール進捗を確認。
    *   **自動更新:** インストール完了時に自動的にモデル一覧を更新。
    *   **エラーハンドリング:** インストール失敗時のエラーメッセージ表示。

---



### 4. セットアップスクリプト仕様

#### 6.1. `scripts/setup-dev.sh`（開発環境セットアップスクリプト）

**目的:**
開発者が初回環境構築時に実行するインタラクティブなスクリプト。**使用するLLM環境を選択**させ、Docker環境の構築と接続テストまでを自動化する。

**実行手順:**
1.  **LLM環境の選択 (対話式):**
    *   スクリプト実行時、ユーザーに以下の選択肢を提示する。
        1.  `Ollama (Dockerで自動セットアップ)`
        2.  `LM Studio (手動でアプリを起動)`
        3.  `スキップ (後で手動設定)`
    *   ユーザーの選択に応じて、以降のDockerコマンドやテスト内容が分岐する。

2.  **前提条件チェック:**
    *   DockerおよびDocker Composeがインストールされ、正常に動作しているかを確認する。
    *   システムで使用する主要ポート（5432, 6379, 8000, 3000）に競合がないかチェックする。
    *   **NVIDIA Container Toolkitのインストール確認:** GPUを使用したLLM処理を最適化するため、NVIDIA Container Toolkitが適切にインストールされているかを確認する。未インストールの場合は、GPU利用の利点とインストール手順を案内する。
    *   **NVIDIA Container Toolkitインストール手順（参考）:**
        - Ubuntu/Debian: `sudo apt update && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg`
        - CentOS/RHEL: `sudo yum install -y nvidia-container-toolkit`
        - 設定後、Dockerデーモンの再起動が必要: `sudo systemctl restart docker`
    *   Ollama選択時は、上記の確認を行い、GPUが利用可能な場合はより高速なLLM推論が期待できることを案内する。

3.  **環境設定:**
    *   プロジェクトルートに `.env.example` から `.env` ファイルを自動生成する。
    *   JWTの秘密鍵やデータベースの認証情報などをランダムな値で初期設定する。
    *   ユーザーが選択したLLM環境に応じて、接続先のデフォルト値（Ollama: `http://localhost:11434`, LM Studio: `http://localhost:1234`）を`.env`ファイルに書き込む。

4.  **Docker環境構築:**
    *   **Ollamaを選択した場合:** `docker-compose --profile ollama up -d --build` コマンドを実行し、基本サービス群に加えてOllamaサービスも起動する。
    *   **LM Studioまたはスキップを選択した場合:** `docker-compose up -d --build` コマンドを実行し、基本サービス群のみを起動する。
    *   バックエンドコンテナ内で、データベースマイグレーションを自動実行する。

5.  **LLM接続テスト:**
    *   ユーザーが選択したLLM環境（OllamaまたはLM Studio）に対してのみ接続テストを実行する。
    *   接続に成功した場合、利用可能なモデル一覧を取得し、ターミナルに表示する。
    *   **Ollama/LM Studio選択時の自動モデル管理:** 指定モデルが未インストールの場合、自動でダウンロード・インストールを実行する。
    *   接続に失敗した場合は、エラーメッセージと共に、ユーザーが確認すべき事項を案内する。（例:「LM Studioアプリを起動してください」「`docker-compose logs ollama`でOllamaコンテナのログを確認してください」）

**出力例 (Ollama選択時):**
```bash
$ ./scripts/setup-dev.sh
どのLLM実行環境を使用しますか？
1) Ollama (Dockerで自動セットアップ)
2) LM Studio (手動でアプリを起動)
3) スキップ (後で手動設定)
#? 1

✅ Docker環境チェック完了
📝 .envファイルを生成しました
🐳 Docker環境とOllamaサービスを構築中... (`--profile ollama` を使用)
✅ PostgreSQL接続確認完了
✅ Redis接続確認完了
🤖 LLM接続テスト中 (Ollama)...
  - Ollama (localhost:11434): ✅ 接続成功
🔍 Checking for llama3 model...
📥 llama3 model not found. Downloading...
   This may take several minutes depending on your internet connection.
✅ llama3 model downloaded successfully!
    利用可能モデル: llama3, codellama
🚀 開発環境構築完了！
   Frontend: http://localhost:3000
   Backend API: http://localhost:8000
```

#### 6.2. `scripts/setup-prod.sh`（本番環境セットアップスクリプト）

**目的:**
本番環境での安全なデプロイを支援するスクリプト。手動操作によるミスを減らし、セキュリティ設定とパフォーマンス最適化を体系的に行う。

**実行手順:**
1.  **セキュリティチェック:**
    *   SSL証明書が指定されたパスに存在するか確認する。
    *   本番用の環境変数がデフォルト値のままでないか、また十分な強度を持つかチェックする。
    *   ファイアウォールが有効であり、必要なポート（80, 443）のみが公開されているか確認する。
2.  **本番環境構築:**
    *   `docker-compose.prod.yml` など、本番用に最適化されたComposeファイルを使用してコンテナを起動する。
    *   nginxリバースプロキシのコンテナを起動し、SSL終端と適切なルーティングを設定する。
    *   Dockerのログローテーション設定を適用する。
3.  **パフォーマンス最適化:**
    *   PostgreSQLのインデックスを最適化するコマンドを実行する。
    *   Redisのメモリ使用量上限など、本番に適した設定を適用する。
    *   アプリケーションキャッシュをクリアし、最新の状態で起動させる。
4.  **監視・ヘルスチェック:**
    *   デプロイ後、各サービスのヘルスチェックエンドポイントにリクエストを送信し、正常な応答が返ってくるか確認する。
    *   ログ監視ツールやエラー監視ツールとの連携設定が正しく行われているか確認する。

---

### 5. API仕様

#### 7.1. LLMモデル管理API

**ベースパス:** `/api/chat`

##### モデル一覧取得
```
GET /api/chat/models?provider=ollama
Response: {
  provider: "ollama",
  models: [
    {
      name: "llama3:latest",
      size: "4.7GB",
      modified: "2024-01-01T00:00:00Z",
      digest: "sha256:...",
      details: {...}
    }
  ]
}
```

##### モデルインストール開始
```
POST /api/chat/models/install
Body: { model_name: "llama3", provider: "ollama" }
Response: { task_id: "uuid", status: "pending" }
```

##### インストール進行状況確認
```
GET /api/chat/models/install/{task_id}
Response: {
  task_id: "uuid",
  model_name: "llama3",
  provider: "ollama",
  status: "running",
  progress: 0.45,
  message: "Downloading...",
  created_at: "2024-01-01T00:00:00Z",
  updated_at: "2024-01-01T00:00:30Z"
}
```

##### モデル削除
```
DELETE /api/chat/models/{model_name}?provider=ollama
Response: { success: true, message: "Model deleted successfully" }
```

---

### 6. 開発ロードマップ（フェーズ別実装計画）

開発計画書【@docs/project-plan.md】に従って開発すること。

---

### 7. Claude Codeへの具体的な指示プロンプト例

#### 8.1. プロジェクト開始時の指示
```prompt
これから「LocalAgentWeaver」というWEBアプリケーションを開発します。
【@CLAUDE.md】をあなたの知識ベースとして、開発を行ってください。

最初に以下のタスク分割を行い、作業の計画と進捗管理ができる【作業計画・進捗管理】を作成してください。
【作業計画・進捗管理】は【@docs/*】に配置して、バージョン管理してください。

あなたは、【作業計画・進捗管理】のタスクに従って、テスト駆動開発（TDD）の原則に従って、高品質で保守性の高いコードで実装してください。

```