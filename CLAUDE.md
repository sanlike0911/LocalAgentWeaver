## **LocalAgentWeaver システム開発設計書 兼 Claude Code実装指示書**

### 1. プロジェクト概要

#### 1.1. プロジェクト名
LocalAgentWeaver

#### 1.2. コンセプト
ローカル環境で動作するLLM（Ollama/LM Studio）と連携し、チームでのドキュメントベースの対話やアイデア創出を支援する、セキュアなAIエージェントプラットフォーム。ユーザーはプロジェクトを作成し、ドキュメントをアップロードし、AIエージェントからなる「チーム」と対話することで、高度な情報分析やコンテンツ生成を行う。

#### 1.3. 主要機能
- ユーザー認証・管理
- プロジェクト管理
- LLM機能
  - ローカルLLM（Ollama/LM Studio）との連携機能
  - ローカルLLM（Ollama/LM Studio）の**LLMモデル自動管理機能**（未インストールモデルの自動ダウンロード・インストール）
- チャット機能: ユーザーがフロントエンドで質問を送信します。
  - 質問のベクトル化: ユーザーの質問も、ドキュメントと同じ埋め込みモデルを使ってベクトルに変換されます。これにより、質問とドキュメントの内容を同じ基準で比較する。
  - 類似度検索: サーバーが質問のベクトルを使い、現在のワークスペースのベクトルデータベースに対して類似度検索を実行します。データベースは、質問のベクトルと意味的に最も近いテキストチャンクを複数返す。
  - コンテキストの拡張: 検索されたテキストチャンクは、ユーザーの元の質問と組み合わされ、LLMへの指示（プロンプト）が拡張されます。これにより、LLMは回答の根拠となる情報を手に入れます。
  - LLMへの問い合わせ: 拡張されたプロンプトが、選択されたLLMに送信する。
  - 回答の生成: LLMは与えられたコンテキストに基づいて、AIエージェントチームが質問に対する回答を生成します。
  - ユーザーへの表示: 生成された回答が、出典元となったドキュメントの引用情報とともに、チャットインターフェースに表示されます。
- RAG機能（Retrieval-Augmented Generation）
  - ドキュメントのアップロード。システム設定として、プロジェクト単位にアップロード件数「default: 100件」、ファイルサイズの上限を「default: 30MB」とする。
  - 対応ファイル形式：TXT、MD、PDF、EXCEL、WORD、POWERPOINT
  - テキスト抽出：アップロードされたファイルはコレクターサービスで、敵kストコンテンツとして抽出する。
  - チャンキング（分割）：抽出されたテキストは、扱いやすいように小さな塊（チャンク）に分割されます。これは、情報を効率的にベクトル化し、検索精度を高めるための重要なステップ。
  - 埋め込み（ベクトル化）：各テキストチャンクが選択された埋め込みモデルに渡され、その意味内容を表すベクトルに変換されます。
  - ベクトル格納: 生成されたベクトルは、元のテキストチャンクや関連メタデータと共に、指定されたベクトルデータベースに保存されます。
- AIエージェントチームの作成・管理・選択
- プロジェクトの共有機能

#### 1.4. 実装上の共通指針（コーディング規約）

- **開発手法:** **テスト駆動開発（TDD）**を厳守する。機能実装前にテストコードを作成すること。
- **認証:** メールアドレスとパスワードによる認証。認証トークンには**JWT**を使用する。
- **LLMプロバイダー:** **Ollama**および**LM Studio**に対応。設定ファイル (`.env`) とUIの両方から接続先を切り替え可能な設計とする。
- **LLMモデル管理:** 未インストールモデルの自動検出・ダウンロード・インストール機能を提供。進行状況をリアルタイム表示する。
- **設定管理:** DB接続情報、LLM接続先、ファイルアップロード上限（デフォルト30MB）などの設定値は、環境変数 (`.env`) で管理する。
- **デプロイ:** **Docker**および**docker-compose**を前提とする。Windows/Linuxの両環境で動作すること。
- **ログ:** **構造化ログ**を出力する。リクエストIDなどを付与し、処理の追跡を容易にすること。
- **コード品質:** 可読性と保守性を最優先する。型ヒントを徹底し、静的解析ツールを導入する。
- **セキュリティ:** ファイルアップロード時のMIMEタイプ検証、APIへのCORS設定とレート制限、ユーザー入力のサニタイズ・バリデーションを行う。

---

### 2. システムアーキテクチャ

#### 2.1. システム構成図
LocalAgentWeaverは、フロントエンド、バックエンド、データベース、そしてユーザーのローカル環境で動作するLLM実行環境から構成されます。バックエンドは、ビジネスロジックの中核を担い、LLM実行環境へのリクエストを中継・管理します。

```
 +-----------+      +----------------------+      +------------------+
 |           | HTTP |                      |      |                  |
 |  ユーザー   |----->|   Frontend (Next.js) |----->|  Backend (FastAPI) |
 | (ブラウザ)  |      | (UI/UX)              |      | (ビジネスロジック) |
 +-----------+      +-----------+----------+      +---------+--------+
                                |                          |
            (WebSocketリアルタイム通信)                      | (API Call)
                                |                          |
                                |          +---------------v----------------+
                                |          |                                |
                                |          |    LLM実行環境 (ローカル)      |
                                +--------->|     - Ollama                   |
                                           |     - LM Studio                |
                                           |                                |
                                           +--------------------------------+
                                                           ^
                                                           |
                                       +-------------------+--------------------+
                                       |                   |                    |
                              +--------v---------+ +-------v--------+
                              |                  | |                |
                              |    PostgreSQL    | |      Redis     |
                              |  (データ永続化)  | | (キャッシュ)   |
                              +------------------+ +----------------+
```

#### 2.2. 技術スタック
| カテゴリ | 技術選定 | 理由 |
| :--- | :--- | :--- |
| **バックエンド** | Python, FastAPI | LLMとの親和性が高く、高性能。WebSocketによるリアルタイム通信に対応。 |
| **フロントエンド** | TypeScript, React, Next.js | 型安全で堅牢な開発が可能。SSR/SSGによる高いパフォーマンスを実現。 |
| **データベース** | PostgreSQL | リレーショナルデータの永続化に適した、信頼性の高いOSS RDBMS。 |
| **キャッシュ** | Redis | セッション管理や高速なデータキャッシュによるパフォーマンス向上。 |
| **UIフレームワーク** | Tailwind CSS, Shadcn/ui | 高いカスタマイズ性とコンポーネント指向による効率的なUI開発。 |
| **LLM実行環境** | **Ollama / LM Studio (設定により切替可能)** | **ローカル環境でセキュアにLLMを実行するため。ユーザーの既存環境や好みのモデルに応じて選択肢を提供。** |
| **コンテナ化** | Docker, Docker Compose | 開発・本番環境の差異をなくし、ポータビリティを確保。 |
| **テスト** | Pytest (BE), Jest/Playwright (FE) | TDDを実践し、コード品質を担保するための標準的なテストフレームワーク。|

#### 2.3. `docker-compose.yml` の構成
プロジェクトのサービス群は`docker-compose.yml`で定義される。特にLLM実行環境として、OllamaはDockerサービスとして管理可能とする。

```yaml
# docker-compose.yml の構成概要
version: '3.8'
services:
  backend: ...
  frontend: ...
  db: ...
  redis: ...

  # プロファイル機能により、選択的に起動されるOllamaサービス
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
    # GPUを利用する場合の推奨設定 (NVIDIA Container Toolkitが必要)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - ollama # '--profile ollama' オプションで起動
```

#### 2.4. ファイル・ディレクトリ構造
フィーチャーベースのディレクトリ構造を採用し、機能ごとの独立性と保守性を高める。

```plaintext
LocalAgentWeaver/
├── backend/
│   ├── app/
│   │   ├── features/          # 機能ごとのモジュール (auth, projects, teams, chat)
│   │   ├── core/              # 設定、DB接続など共通のコア機能
│   │   ├── models/            # SQLAlchemyのデータモデル定義
│   │   └── tests/             # テストコード
│   ├── requirements.txt
│   └── Dockerfile
├── frontend/
│   ├── src/
│   │   ├── features/          # 機能ごとのコンポーネント、フック、APIクライアント
│   │   ├── components/ui/     # Shadcn/uiによる共通UIコンポーネント
│   │   ├── hooks/             # 共通カスタムフック
│   │   └── utils/             # 共通ユーティリティ関数
│   ├── package.json
│   └── Dockerfile
├── scripts/                   # セットアップ用スクリプト
│   ├── setup-dev.sh
│   └── setup-prod.sh
├── .env.example               # 環境変数テンプレート
└── docker-compose.yml         # 開発・本番環境定義
```

---

### 3. UI/UX設計 (詳細仕様)

**基本方針:** 本システムのUIは、ドキュメントという「コンテキスト」を管理しながら、AIチームと「対話」することに最適化された設計とする。複雑な操作を排し、チーム選択や編集といった主要なワークフローを直感的かつ情報豊かに提供する。

#### 3.1. 画面構成図

```
                                      +--------------------------------+
                                      |                                |
[未ログイン] -------------------------> |  ログイン / 新規登録画面        |
                                      |  (LAW-SC-010)                  |
                                      +---------------+----------------+
                                                      | (認証成功)
                                                      |
                          +---------------------------v----------------------------+
                          |                                                        |
                          |      ダッシュボード (プロジェクト一覧画面)             |
                          |      (LAW-SC-100)                                      |
                          |                                                        |
                          +---------------------------+----------------------------+
                                                      | (プロジェクト選択)
                                                      |
+-----------------------------------------------------v------------------------------------------------------+
|                                                                                                              |
|                                       メイン画面 (チャット / プロジェクト編集)                               |
|                                       (LAW-SC-200)                                                           |
|                                                                                                              |
+-----------------------------------------------------+--------------------------------------------------------+
              | (左サイドバーのチーム名をクリック)              ^
              |                                               | (チーム選択後)
              v                                               |
+---------------------------+---------------------------------+---------------------------+
|                           |                                 |                           |
|  チーム選択モーダル       | <--(新規作成/編集)-->            |  チーム編集モーダル       |
|  (LAW-SC-215)             |                                 |  (LAW-SC-210)             |
|                           |                                 |                           |
+---------------------------+---------------------------------+---------------------------+
              ^                                               
              | (Modelボタンクリック)                         
              |                                               
+---------------------------+                               
|                           |                               
|  モデル選択モーダル       |                               
|  (LAW-SC-220)             |                               
|                           |                               
+---------------------------+
```

#### 3.2. 各画面仕様

##### 3.2.1. ログイン / 新規登録画面 - `LAW-SC-010`
*   **概要:** ユーザー認証を行うためのエントリーポイント。タブでログインと新規登録を切り替えるシンプルなフォーム。

##### 3.2.2. ダッシュボード（プロジェクト一覧）画面 - `LAW-SC-100`
*   **概要:** ユーザーが関わるプロジェクトを管理するハブ画面。プロジェクトをカード形式で一覧表示し、新規作成や検索が可能。

##### 3.2.3. メイン画面（チャット / プロジェクト編集） - `LAW-SC-200`
*   **概要:** アプリケーションの中核画面。3カラムレイアウトを採用。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------------+
    |  [←] プロジェクト名 - プロジェクト説明                                             |
    +--------------------------------------------------------------------------------+
    | Left Sidebar     |        Center Chat Area       |   Right Sidebar           |
    | (320px)          |         (Flex-1)               |   (256px)                 |
    |                  |                                |                           |
    | [ チーム選択 ]   |  [ AI Assistant ]  [ Provider ][ Model ]                  |
    | 👥 選択中のチーム    |   ┌─ Ollama ▼─┐   [ llama3 ]  | 📊 LLM接続状態            |
    | 説明文...         |   └─────────────┘             | ○ Ollama: 接続中         |
    | [ ⚙️ 設定 ]       |                                | 📋 利用可能モデル:        |
    |                  |  ┌─ Chat Messages Area ─────┐  |   llama3, codellama     |
    | [ ドキュメント ]     |  │ 💬 User: こんにちは       │  |                           |
    | [📄 Upload ]      |  │ 🤖 AI: こんにちは!         │  | 📚 引用・出典             |
    |                  |  │                            │  |                           |
    | 📁 Drop Zone     |  │ ...                        │  | AIの応答に関連する       |
    | ファイルをD&D...   |  │                            │  | ドキュメントの引用が     |
    |                  |  │                            │  | ここに表示されます       |
    | 📋 Document List |  └────────────────────────────┘  |                           |
    | ✅ doc1.pdf      |                                |                           |
    | ❌ doc2.txt      |  [ メッセージ入力欄...    ] [送信]  |                           |
    +------------------+--------------------------------+---------------------------+
    ```
*   **左サイドバー (320px):**
    *   **チーム選択エリア:** 現在のチーム名と説明を表示するクリック可能なエリア。クリックで**チーム選択モーダル (`LAW-SC-215`)** を開く。
    *   **ドキュメント管理:** ファイルアップロード、ドラッグ&ドロップエリア、コンテキストに含めるかの有効/無効切り替え。
*   **中央エリア (Flex-1):** 
    *   **ヘッダー:** LLMプロバイダー選択（Ollama/LM Studio）、モデル選択
    *   **チャット履歴:** ユーザーとAIの対話履歴
    *   **応答時間表示:** チャット入力からLLMの応答が完了するまでの経過時間をタイムスタンプと同一行に「21:15（応答時間：23.4秒）」の形式で表示。1秒未満は小数点表示、1分未満は秒のみ、1時間未満は分秒、1時間以上は時分秒で表示
    *   **プロンプト入力欄:** メッセージ入力と送信ボタン
*   **右サイドバー (256px):**
    *   **LLM接続状態:** 現在のプロバイダーの接続状況と利用可能モデル一覧
    *   **引用・出典表示:** AIの応答に関連するドキュメントの引用情報

##### 3.2.4. チーム選択モーダル - `LAW-SC-215`
*   **概要:** チームの概要を確認しながら、プロジェクトで使用するAIチームを選択するモーダル。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ チームを選択 ]                                                          |
    |--------------------------------------------------------------------------|
    |  [ 🔍 チームを検索... ]  [ + 新しいチームを作成 ]                          |
    |--------------------------------------------------------------------------|
    |  +--------------------------------------------------------------------+  |
    |  | [ 開発チーム ] (現在選択中)                             [ ⚙️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | 役割: ソフトウェア開発に関する質問に答える専門家チームです。         |  |
    |  | エージェント: [ シニアデベロッパー ] [ QAエンジニア ]                |  |
    |  +--------------------------------------------------------------------+  |
    |  ... (他のチームタイル) ...                                             |
    |--------------------------------------------------------------------------|
    |                                                         [ 閉じる ]       |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   **チームタイルクリック:** チームを選択しモーダルを閉じる。
    *   **`+ 新しいチームを作成` ボタン:** `LAW-SC-210`を新規作成モードで開く。
    *   **`⚙️` (編集) ボタン:** `LAW-SC-210`を編集モードで開く。

##### 3.2.5. チーム編集モーダル - `LAW-SC-210`
*   **概要:** AIエージェントを組み合わせて独自のチームを新規作成、または編集するモーダル。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ チームを編集 ]                                             [ × ]      |
    |--------------------------------------------------------------------------|
    |  チーム名: [ input: 開発チーム ]                                         |
    |  チームの役割・目的: [ textarea: このチームは... ]                       |
    |  ----------------------------------------------------------------------  |
    |  [ エージェント ]                                                        |
    |  +--------------------------------------------------------------------+  |
    |  | [エージェント 1] [🗑️] エージェント名: [ input ] 役割: [ textarea ]  |  |
    |  +--------------------------------------------------------------------+  |
    |  [ + エージェントを追加 ]                                                |
    |  ----------------------------------------------------------------------  |
    |  [ 他のチームからエージェントをコピー ]                                  |
    |  コピー元チーム: [ 事業戦略チーム ▼ ]                                    |
    |  +--------------------------------------------------------------------+  |
    |  | 役割: 新規事業の立案と市場分析を行います... (リードオンリー)         |  |
    |  +--------------------------------------------------------------------+  |
    |  エージェント: [ ✅ マーケター ] [ ✅ データアナリスト ]                 |
    |  [ 選択したエージェントをコピー ]                                        |
    |--------------------------------------------------------------------------|
    |  [ キャンセル ]                                    [ 保存 ]              |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   チームの基本情報（名称、役割）を定義。
    *   エージェントを動的に追加・削除。
    *   他のチームからエージェントを役割を確認しながらコピー。

##### 3.2.6. モデル選択モーダル - `LAW-SC-220`
*   **概要:** LLMモデルを選択・管理するためのモーダル。インストール済みモデルの選択、新しいモデルのインストール、モデルの削除が可能。
*   **ワイヤーフレーム:**
    ```
    +--------------------------------------------------------------------------+
    |  [ Ollama モデル選択 ]                                      [ × ]      |
    |--------------------------------------------------------------------------|
    |                                                                          |
    |  [ インストール済みモデル ]                                              |
    |  +--------------------------------------------------------------------+  |
    |  | [ llama3.2:1b ] (現在選択中)                          [ 🗑️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | サイズ: 1.3GB　更新: 17時間前                                     |  |
    |  +--------------------------------------------------------------------+  |
    |  | [ codellama:7b ]                                       [ 🗑️ ]      |  |
    |  |--------------------------------------------------------------------|  |
    |  | サイズ: 3.8GB　更新: 2日前                                        |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |  [ 推奨モデル ]                                                          |
    |  +--------------------------------------------------------------------+  |
    |  | [ llama3 ]                                            [ ⬇️ インストール ] |
    |  |--------------------------------------------------------------------|  |
    |  | Meta の Llama 3 モデル                                             |  |
    |  +--------------------------------------------------------------------+  |
    |  | [ mistral ]                                           [ ⬇️ インストール ] |
    |  |--------------------------------------------------------------------|  |
    |  | Mistral AI の高性能モデル                                          |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |  [ インストール状況 ]                                                    |
    |  +--------------------------------------------------------------------+  |
    |  | [ gemma ] ⏳ インストール中...                                      |  |
    |  |--------------------------------------------------------------------|  |
    |  | ████████████████░░░░ 80% ダウンロード中...                        |  |
    |  +--------------------------------------------------------------------+  |
    |                                                                          |
    |--------------------------------------------------------------------------|
    |  [ 更新 ]                                              [ 閉じる ]       |
    +--------------------------------------------------------------------------+
    ```
*   **機能:**
    *   **インストール済みモデル表示:** 現在利用可能なモデルを一覧表示。選択中のモデルを強調表示。
    *   **モデル選択:** クリックでモデルを選択・切り替え。
    *   **モデル削除:** 各モデルの削除ボタンで不要なモデルを削除（確認ダイアログ表示）。
    *   **推奨モデル表示:** プロバイダーごとの推奨モデルを表示し、ワンクリックでインストール開始。
    *   **リアルタイムインストール進捗:** インストール中のモデルの進捗をプログレスバーで表示。
    *   **インストール状況管理:** 複数モデルの同時インストールに対応し、各状況を個別に表示。
*   **状態管理:**
    *   **インストールタスク監視:** バックグラウンドで2秒間隔でインストール進捗を確認。
    *   **自動更新:** インストール完了時に自動的にモデル一覧を更新。
    *   **エラーハンドリング:** インストール失敗時のエラーメッセージ表示。

---



### 4. セットアップスクリプト仕様

#### 6.1. `scripts/setup-dev.sh`（開発環境セットアップスクリプト）

**目的:**
開発者が初回環境構築時に実行するインタラクティブなスクリプト。**使用するLLM環境を選択**させ、Docker環境の構築と接続テストまでを自動化する。

**実行手順:**
1.  **LLM環境の選択 (対話式):**
    *   スクリプト実行時、ユーザーに以下の選択肢を提示する。
        1.  `Ollama (Dockerで自動セットアップ)`
        2.  `LM Studio (手動でアプリを起動)`
        3.  `スキップ (後で手動設定)`
    *   ユーザーの選択に応じて、以降のDockerコマンドやテスト内容が分岐する。

2.  **前提条件チェック:**
    *   DockerおよびDocker Composeがインストールされ、正常に動作しているかを確認する。
    *   システムで使用する主要ポート（5432, 6379, 8000, 3000）に競合がないかチェックする。
    *   **NVIDIA Container Toolkitのインストール確認:** GPUを使用したLLM処理を最適化するため、NVIDIA Container Toolkitが適切にインストールされているかを確認する。未インストールの場合は、GPU利用の利点とインストール手順を案内する。
    *   **NVIDIA Container Toolkitインストール手順（参考）:**
        - Ubuntu/Debian: `sudo apt update && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg`
        - CentOS/RHEL: `sudo yum install -y nvidia-container-toolkit`
        - 設定後、Dockerデーモンの再起動が必要: `sudo systemctl restart docker`
    *   Ollama選択時は、上記の確認を行い、GPUが利用可能な場合はより高速なLLM推論が期待できることを案内する。

3.  **環境設定:**
    *   プロジェクトルートに `.env.example` から `.env` ファイルを自動生成する。
    *   JWTの秘密鍵やデータベースの認証情報などをランダムな値で初期設定する。
    *   ユーザーが選択したLLM環境に応じて、接続先のデフォルト値（Ollama: `http://localhost:11434`, LM Studio: `http://localhost:1234`）を`.env`ファイルに書き込む。

4.  **Docker環境構築:**
    *   **Ollamaを選択した場合:** `docker-compose --profile ollama up -d --build` コマンドを実行し、基本サービス群に加えてOllamaサービスも起動する。
    *   **LM Studioまたはスキップを選択した場合:** `docker-compose up -d --build` コマンドを実行し、基本サービス群のみを起動する。
    *   バックエンドコンテナ内で、データベースマイグレーションを自動実行する。

5.  **LLM接続テスト:**
    *   ユーザーが選択したLLM環境（OllamaまたはLM Studio）に対してのみ接続テストを実行する。
    *   接続に成功した場合、利用可能なモデル一覧を取得し、ターミナルに表示する。
    *   **Ollama/LM Studio選択時の自動モデル管理:** 指定モデルが未インストールの場合、自動でダウンロード・インストールを実行する。
    *   接続に失敗した場合は、エラーメッセージと共に、ユーザーが確認すべき事項を案内する。（例:「LM Studioアプリを起動してください」「`docker-compose logs ollama`でOllamaコンテナのログを確認してください」）

**出力例 (Ollama選択時):**
```bash
$ ./scripts/setup-dev.sh
どのLLM実行環境を使用しますか？
1) Ollama (Dockerで自動セットアップ)
2) LM Studio (手動でアプリを起動)
3) スキップ (後で手動設定)
#? 1

✅ Docker環境チェック完了
📝 .envファイルを生成しました
🐳 Docker環境とOllamaサービスを構築中... (`--profile ollama` を使用)
✅ PostgreSQL接続確認完了
✅ Redis接続確認完了
🤖 LLM接続テスト中 (Ollama)...
  - Ollama (localhost:11434): ✅ 接続成功
🔍 Checking for llama3 model...
📥 llama3 model not found. Downloading...
   This may take several minutes depending on your internet connection.
✅ llama3 model downloaded successfully!
    利用可能モデル: llama3, codellama
🚀 開発環境構築完了！
   Frontend: http://localhost:3000
   Backend API: http://localhost:8000
```

#### 6.2. `scripts/setup-prod.sh`（本番環境セットアップスクリプト）

**目的:**
本番環境での安全なデプロイを支援するスクリプト。手動操作によるミスを減らし、セキュリティ設定とパフォーマンス最適化を体系的に行う。

**実行手順:**
1.  **セキュリティチェック:**
    *   SSL証明書が指定されたパスに存在するか確認する。
    *   本番用の環境変数がデフォルト値のままでないか、また十分な強度を持つかチェックする。
    *   ファイアウォールが有効であり、必要なポート（80, 443）のみが公開されているか確認する。
2.  **本番環境構築:**
    *   `docker-compose.prod.yml` など、本番用に最適化されたComposeファイルを使用してコンテナを起動する。
    *   nginxリバースプロキシのコンテナを起動し、SSL終端と適切なルーティングを設定する。
    *   Dockerのログローテーション設定を適用する。
3.  **パフォーマンス最適化:**
    *   PostgreSQLのインデックスを最適化するコマンドを実行する。
    *   Redisのメモリ使用量上限など、本番に適した設定を適用する。
    *   アプリケーションキャッシュをクリアし、最新の状態で起動させる。
4.  **監視・ヘルスチェック:**
    *   デプロイ後、各サービスのヘルスチェックエンドポイントにリクエストを送信し、正常な応答が返ってくるか確認する。
    *   ログ監視ツールやエラー監視ツールとの連携設定が正しく行われているか確認する。

---

### 5. API仕様

#### 7.1. LLMモデル管理API

**ベースパス:** `/api/chat`

##### モデル一覧取得
```
GET /api/chat/models?provider=ollama
Response: {
  provider: "ollama",
  models: [
    {
      name: "llama3:latest",
      size: "4.7GB",
      modified: "2024-01-01T00:00:00Z",
      digest: "sha256:...",
      details: {...}
    }
  ]
}
```

##### モデルインストール開始
```
POST /api/chat/models/install
Body: { model_name: "llama3", provider: "ollama" }
Response: { task_id: "uuid", status: "pending" }
```

##### インストール進行状況確認
```
GET /api/chat/models/install/{task_id}
Response: {
  task_id: "uuid",
  model_name: "llama3",
  provider: "ollama",
  status: "running",
  progress: 0.45,
  message: "Downloading...",
  created_at: "2024-01-01T00:00:00Z",
  updated_at: "2024-01-01T00:00:30Z"
}
```

##### モデル削除
```
DELETE /api/chat/models/{model_name}?provider=ollama
Response: { success: true, message: "Model deleted successfully" }
```

---

### 6. 開発ロードマップ（フェーズ別実装計画）

開発計画書【@docs/project-plan.md】に従って開発すること。

---

### 7. Claude Codeへの具体的な指示プロンプト例

#### 8.1. プロジェクト開始時の指示
```prompt
これから「LocalAgentWeaver」というWEBアプリケーションを開発します。
【@CLAUDE.md】をあなたの知識ベースとして、開発を行ってください。

最初に以下のタスク分割を行い、作業の計画と進捗管理ができる【作業計画・進捗管理】を作成してください。
【作業計画・進捗管理】は【@docs/*】に配置して、バージョン管理してください。

あなたは、【作業計画・進捗管理】のタスクに従って、テスト駆動開発（TDD）の原則に従って、高品質で保守性の高いコードで実装してください。

```